---
title: "Logistic Regression Model"
output: html_notebook
---

## Final Models 

These are the models that are the most relevant to the results. Below this I tried other models, but found that none of the variables beyond ID were helpful for predicting whether a participant missed a session or not. 

```{r}

base.lame = glmer(missed ~  (1|ID),
   data = log.df, family = binomial)

meant.lame = glmer(missed ~ mean_trust + (1|ID),
   data = log.df, family = binomial)

meanSSE.lame = glmer(missed ~ mean_SSE + (1|ID),
   data = log.df, family = binomial)

compare_performance(base.lame, meant.lame, meanSSE.lame, rank = TRUE)

```


```{r}
z.df = log.df %>%
  select(ID, mean_trust, missed)

z.df
```

```{r}
#specify the cross-validation method
# ctrl <- trainControl(method = "LOOCV", ,
#   method = "glm",
#   family = "binomial")
# 
# #fit a regression model and use LOOCV to evaluate performance
# model <- train(missed ~ mean_trust, data = z.df, family = binomial(), method = "lm", trControl = ctrl, na.action=na.exclude)
# 
# print(model)

#Source: https://stackoverflow.com/questions/18449013/r-logistic-regression-area-under-curve
model = glmer(missed ~ mean_trust + (1|ID) , data=z.df, family=binomial(link="logit"), na.action=na.omit) # this just calculates the probabilities

```


```{r}
# summary(model)
prob=predict(model,type=c("response"))
z.df$prob=prob
prob
z.df
```


```{r}

pred <- prediction(prob, z.df$missed) 
perf <- performance(pred, measure = "tpr", x.measure = "fpr")     
plot(perf, col=rainbow(7), main="ROC curve Admissions", xlab="Specificity", 
     ylab="Sensitivity")    
abline(0, 1) #add a 45 degree line

```

```{r}
prediction(prob, z.df$missed)
```


## Other Models

Other models that confirm that none of the variables are very good at predicting whether ap participant would miss a session or not. 

```{r}
base.lame = glmer(missed ~  (1|ID),
   data = log.df, family = binomial)
#event.lame = glmer(missed ~  event + (1|ID), # removing event because it is not independent 
#  data = log.df, family = binomial)
efficacy.lame = glmer(missed ~  efficacy_value + (1|ID),
   data = log.df, family = binomial)
propensity.lame = glmer(missed ~  propensity_value + (1|ID),
   data = log.df, family = binomial)
complacency.lame = glmer(missed ~  complacency_value + (1|ID),
   data = log.df, family = binomial)
# exposure.lame = glmer(missed ~  exposure + (1|ID),
#    data = log.df, family = binomial) #removing exposure because not independent 
mean_trust.lame = glmer(missed ~  mean_trust + (1|ID),
   data = log.df, family = binomial)
mean_SSE.lame = glmer(missed ~  mean_SSE + (1|ID),
   data = log.df, family = binomial)


compare_performance(base.lame, efficacy.lame, propensity.lame, complacency.lame, mean_trust.lame, mean_SSE.lame,  rank = TRUE)

```



```{r}

base.lame = glmer(missed ~  (1|ID),
   data = log.df, family = binomial)
ecp.lame = glmer(missed ~  complacency_value*propensity_value + (1|ID),
   data = log.df, family = binomial)

ece.lame = glmer(missed ~  complacency_value*efficacy_value + (1|ID),
   data = log.df, family = binomial)

epe.lame = glmer(missed ~  propensity_value*efficacy_value + (1|ID),
   data = log.df, family = binomial)

st.lame = glmer(missed ~  mean_trust*mean_SSE + (1|ID),
   data = log.df, family = binomial)

compare_performance(base.lame, ecpm.lame, ece.lame, epe.lame, st.lame, rank = TRUE)

```


## Older Code

I used caret to try to the predictions but lmer made more sense for the purposes of the results section. 

```{r}
# low.df
# high.df
# 
# 
# log.df = merge(low.df, high.df, by = "ID")
# 
# log.df = log.df %>%
#   drop_na() %>%
#   select(-ID) %>%
#   distinct()
# 
# log.df

```


```{r}
## old version of partioning the data

# 
# model.df = f.df %>% mutate(missed = if_else(is.na(f.df$mean_trust_value) == TRUE, 1,0)) #missed an event in a scenario 
# 
# low.df = model.df  %>% #low reliability day df
#   filter(reliability == "low") %>%
#   group_by(ID) #%>%
#   # select(ID, event, session, missed) %>%
#   # mutate(label = if_else(sum(missed)> 0, 1,0))
# 
# trust = model.df %>%
#     filter(reliability == "high") %>%
#   group_by(ID, event, session) %>%
#   summarise(trust = mean(mean_trust_value, na.rm =TRUE))
# 
# ID.feat = model.df %>%
#   select(ID, efficacy_value, propensity_value, complacency_value, event, reliability, exposure) %>%
#   distinct()
# 
# high.df = merge(trust,ID.feat)
# 
# low.df =model.df  %>%
#   filter(reliability == "low") %>%
#   group_by(ID) %>%
#   select(ID, event, session, missed) %>%
#   mutate(missed = if_else(missed == 1, "missed","not_missed")) %>%
#   mutate(missed = as.factor(missed)) %>%
#   mutate(session = if_else(session == "3", 1,2)) 
#   
# log.df = merge(high.df,low.df) 
# 
# levels(log.df$missed) = c("missed", "not_missed")
# 
# log.df = log.df %>%
#   mutate(ID = as.character(ID)) %>%
#   select(-ID)


```


## Partition data into training and testing
```{r partition}
# library(tidyverse)
# library(skimr)
# library(ggforce)
# library(caret) # Tools and common interface to many supervised learning algorithms
# library(patchwork) # For combining multiple plots
# library(plotROC)
# library(pROC)
# 
# ## Creates a random sample of rows for training
# inTrain = createDataPartition(log.df$missed, p = 3/4, list = FALSE)
# 
# 
# ## Create dataframes of descriptive variables for training and testing
# # Slice extracts rows based on vector of row numbers
# trainDescr = log.df%>% slice(inTrain) %>% select(-missed)
# testDescr = log.df %>% slice(-inTrain) %>% select(-missed)
# 
# trainClass = log.df %>% slice(inTrain) %>% select(missed) %>% as.matrix() %>% as.factor()
# testClass = log.df %>% slice(-inTrain) %>% select(missed) %>% as.matrix() %>% as.factor()
# 
# 
# ## Proportion of good and bad cases should be the same in testing and training
# # Ideally the classes should be balanced
# log.df %>% select(missed) %>%  table() %>% prop.table() %>% round(3)*100 
# 
# trainClass %>% table() %>% prop.table() %>% round(3)*100
# 
# testClass %>% table() %>% prop.table() %>% round(3)*100
# 
# trainClass
# testClass
# ```
# 
# 
# 
# 
# 
# ```{r partition}
# ## Trans.mod is a transformation model that is trained and the applied to the data
# Trans.mod = preProcess(trainDescr, method = c("center", "scale")) #centered and scaled data
# trainScaled = predict(Trans.mod, trainDescr) #create the training set
# testScaled = predict(Trans.mod, testDescr) #create the testing set
# 
# ## Plot transformed data      
# raw.plot = ggplot(trainDescr, aes(mean_trust)) + geom_histogram(bins = 60) +
#   labs(title = "Original")
# 
# scaled.plot = ggplot(trainScaled, aes(mean_trust)) + geom_histogram(bins = 60) +
#   labs(title = "Scaled")
# 
# 
# raw.plot
# 
# scaled.plot
# 
# ```
# 
# 
# 
# 
# 
# ```{r tune}
# #setting all of the variables for training to have them be what you want...
# train.control = trainControl(method = "LOOCV",
#                              #, 
#                              # search = "grid", # for tuning hyperparameters
#                               classProbs = TRUE, # return probability of prediction
#                              # savePredictions = "final",
#                              summaryFunction = twoClassSummary
#                              )
# 
# ```
# 
# 
# 
# ```{r}
# # trainScaled = trainScaled %>%
# #   rename(mean_trust_value = trust)
# 
# glm.fit = train(x = trainScaled, y = trainClass,
#    method = 'glm', metric = "ROC",
#    trControl = train.control)
# 
# glm.fit
# ```
# 
# 
# ```{r}
# glm.pred = predict(glm.fit, testScaled)  #originally had a very high rate of false positives 
# 
# confusionMatrix(glm.pred, testClass)
# 
# ```
# 
# ## Assess performance (xgb): ROC plot
# The ROC plot provides a more detailed comparison of models across the of decision thresholds.
# 
# ```{r assess_ROC, warning=FALSE, message= FALSE}
# ## Use model to generate predictions
# glm.pred = predict(glm.fit, testScaled, type = "prob")
# 
# glm.pred
# 
# predicted.log.df = log.df %>% slice(-inTrain) %>%
#   cbind(glm.pred.missed = glm.pred$missed) %>%
#   cbind(obs = testClass)
# 
# 
# ## Calculate ROC coordinates and area under curve (AUC)
# glm.roc = roc(predictor = predicted.log.df$glm.pred, 
#               response = predicted.log.df$obs, 
#               AUC = TRUE, ci = TRUE)
# 
# predicted.log.df
# ```
# 
# ```{r}
# predicted.log.df = predicted.log.df %>%
#   mutate(obs = if_else(obs == "missed", 1,0)) #%>%
#  # mutate(glm.pred.missed = if_else(obs == "missed", 1,0))
# 
# predicted.log.df
# ```
# 
# 
# 
# ```{r}
# glm.roc$auc
# glm.roc$ci
# 
# 
# ## Plot ROC
# xgb_glm.roc.plot = 
# ggplot(data = predicted.log.df, aes(d = obs, m = glm.pred.missed)) + 
#   geom_abline(colour = "grey60") +
#   geom_roc(labels = FALSE, linealpha = .5, pointalpha = .5) + # Labels show the predictor value
#    annotate("text", x = .5, y = .375, hjust = 0,
#            label = paste("AUC(glm) =", round(glm.roc$auc, 2))) +
#   labs(title = "Prediction of missed event in a session on day 2", 
#        subtitle = " predictions (logistic regression)") +
#   coord_equal() +
#   style_roc() 
#   
# xgb_glm.roc.plot
# ggsave("xgb_glm-roc.png", xgb_glm.roc.plot, width = 5, height = 4.5)

```




